{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_preprocessing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "xniwME_IVwyn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_label(idx):\n",
        "    with open('final_output.txt', 'r') as labels:\n",
        "        for line in labels:\n",
        "            number, text = line.split('\\t')\n",
        "            if int(number) == idx:\n",
        "                return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cy7qlNzpVwy8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_filename = 'data.tfrecords'\n",
        "writer = tf.python_io.TFRecordWriter(data_filename)\n",
        "for idx in range (200):\n",
        "    #index = idx\n",
        "    img = load_image(idx)\n",
        "    label = get_label(idx)\n",
        "    feature = {'data/label': _int64_feature(label),\n",
        "               'data/image': _bytes_feature(tf.compat.as_bytes(img.tostring()))}\n",
        "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pLSKDL7xVwzD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import tqdm\n",
        "\n",
        "#The length of the data which cointains the labels        \n",
        "def check_length(file_name):\n",
        "    with open(file_name, 'r') as f:\n",
        "        return sum(1 for line in f)\n",
        "    \n",
        "#Train, Validation and Test data    \n",
        "def divide_data_file(data_file, train_split=0.8, valid_split=0.1):\n",
        "    data_file_dir = os.path.abspath(os.path.dirname(data_file)) #In the data file, there is index and label in a line\n",
        "    length = check_length(data_file) #Length of the data\n",
        "    train_index = int(length * train_split) #Train\n",
        "    valid_index = train_index + int(length * valid_split) #Valid\n",
        "    #Create 3 txt (Train, Valid, Test) and write tehere the appropriate number of the data \n",
        "    with open(data_file, 'r') as df, open(os.path.join(data_file_dir, 'train.txt'), 'w') as train, open(os.path.join(data_file_dir, 'valid.txt'), 'w') as valid, open(os.path.join(data_file_dir, 'test.txt'), 'w') as test:\n",
        "        for index, line in enumerate(df): #Enumerate get an index to every line\n",
        "            if index < train_index:\n",
        "                train.write(line)\n",
        "            elif index < valid_index:\n",
        "                valid.write(line)\n",
        "            else:\n",
        "                test.write(line)\n",
        "    \n",
        "    return train_index, valid_index - train_index, length - valid_index #Return the idnexes of each shards\n",
        "\n",
        "#Loading txt and split every lines according to tabulator\n",
        "def load_file(file_name):\n",
        "    with open(file_name, 'r') as f:\n",
        "        for line in f:\n",
        "           yield line.split('\\t') #yield help us, it remembers the last line, so it can continue where it left it\n",
        "  \n",
        "#Image name equals an index\n",
        "def load_image(idx):\n",
        "    img = plt.imread(os.path.join('D:/Dani/bme/5.félév/dl/hf/full_images', str(idx) + '.jpg'))\n",
        "    if len(img.shape) == 2: #We need to care about black and white pictures, there are only 2 dimesions \n",
        "        img = img[:, :, None]\n",
        "    img = tf.Session().run(tf.image.resize_images(img, [200,200])) #Resize the images\n",
        "    return img\n",
        "\n",
        "#We need to create tfrecords, it will be easier the teching, because of a tensotflow own extension\n",
        "def create_tfrecord_names(tfrecord_path, mode, num_files): #mode (tarin,valid,test), num_files - how much tfrecord should we generate\n",
        "    mode_path = os.path.join(tfrecord_path, mode)\n",
        "    if not os.path.isdir(mode_path): #if there isnt a directory, generate\n",
        "        os.mkdir(mode_path)\n",
        "    return [tf.python_io.TFRecordWriter(os.path.join(mode_path, '{name}.tfrecords'.format(name=index))) for index in range(num_files)] #call teh tfrecord writer\n",
        "      \n",
        "   \n",
        "\n",
        "  def create_tfrecords(data_file, tfrecord_path, max_size=5000, train_split=0.8, valid_split=0.1): #max_size means, that 1 tfrecords file how much data cointains\n",
        "    train_length, valid_length, test_length = divide_data_file(data_file, train_split, valid_split) #call devide_data, we have the shards\n",
        "    data_file_dir = os.path.abspath(os.path.dirname(data_file)) #Source data\n",
        "    \n",
        "    modes = { #3 modes\n",
        "        'train': [ \n",
        "            create_tfrecord_names(tfrecord_path, 'train', math.ceil(train_length / max_size)), #Create a directory for a train tfrecords\n",
        "            os.path.join(data_file_dir, 'train.txt') \n",
        "        ],\n",
        "        'valid': [\n",
        "            create_tfrecord_names(tfrecord_path, 'valid', math.ceil(valid_length / max_size)), #Create a directory for a validation tfrecords\n",
        "            os.path.join(data_file_dir, 'valid.txt')\n",
        "        ],\n",
        "        'test': [\n",
        "            create_tfrecord_names(tfrecord_path, 'test', math.ceil(test_length / max_size)), #Create a directory for a train tfrecords\n",
        "            os.path.join(data_file_dir, 'test.txt')\n",
        "        ]\n",
        "        \n",
        "    }\n",
        "             \n",
        "    for mode in modes:\n",
        "        tfrecords_list = modes[mode][0] #There is a list for tfrecords, for every mode\n",
        "        data_file = modes[mode][1] #There is a data_file, for every mode\n",
        "        prev_tfrecords_index = 0\n",
        "        with tqdm.tqdm() as pbar: #It's an indicator, shows us the actual state of the generate\n",
        "            for index, (image_index, text) in enumerate(load_file(data_file)): #Load file give us from one line the index and the label of the image\n",
        "                pbar.update() #update the indicator\n",
        "                tfrecords_index = index // max_size #eg: amx size is 5000, we have 14970 data, there will be 3 tf records\n",
        "                example = tf.train.Example( #Need an example and features\n",
        "                    features=tf.train.Features(\n",
        "                        feature={'image': tf.train.Feature(float_list=tf.train.FloatList(value=load_image(image_index).reshape(-1))), #Features are - image and text - so create these pairs \n",
        "                                 'text': tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(text)]))}\n",
        "                ))\n",
        "                tfrecords_list[tfrecords_index].write(example.SerializeToString())\n",
        "                if prev_tfrecords_index != tfrecords_index:\n",
        "                    tfrecords_list[prev_tfrecords_index].close()\n",
        "                    prev_tfrecords_index += 1;\n",
        " \n",
        "        \n",
        "create_tfrecords('D:/Dani/bme/5.félév/dl/hf/output.txt', 'D:/Dani/bme/5.félév/dl/hf/tfrecords')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}