{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pLSKDL7xVwzD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tqdm\n",
    "\n",
    "#The length of the data which cointains the indexes and labels of the images       \n",
    "def check_length(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        return sum(1 for line in f)\n",
    "    \n",
    "#Train, Validation and Test data    \n",
    "def divide_data_file(data_file, train_split=0.8, valid_split=0.1):\n",
    "    data_file_dir = os.path.abspath(os.path.dirname(data_file)) #In the data file, index and label are in a line\n",
    "    length = check_length(data_file) #Length of the data\n",
    "    train_index = int(length * train_split) #Train data\n",
    "    valid_index = train_index + int(length * valid_split) #Valid data\n",
    "    #Create 3 txt (Train, Valid, Test) and write there the appropriate number of the data \n",
    "    with open(data_file, 'r') as df, open(os.path.join(data_file_dir, 'train.txt'), 'w') as train, open(os.path.join(data_file_dir, 'valid.txt'), 'w') as valid, open(os.path.join(data_file_dir, 'test.txt'), 'w') as test:\n",
    "        for index, line in enumerate(df): #Enumerate get an index to every line\n",
    "            if index < train_index:\n",
    "                train.write(line)\n",
    "            elif index < valid_index:\n",
    "                valid.write(line)\n",
    "            else:\n",
    "                test.write(line)\n",
    "    \n",
    "    return train_index, valid_index - train_index, length - valid_index #Return the idnexes of each shards\n",
    "\n",
    "#Loading txt and split every lines according to tabulator. index '\\t' text\n",
    "def load_file(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        for line in f:\n",
    "           yield line.split('\\t') #yield helps us, it remembers the last line, so it can continue where it left it\n",
    "#\n",
    "# Getting the image after the index. index => index.jpg\n",
    "def load_image(idx):\n",
    "    img = plt.imread(os.path.join('D:/Dani/bme/5.félév/dl/hf/full_images', str(idx) + '.jpg'))\n",
    "    if len(img.shape) == 2: #We need to care about black and white pictures, there are only 2 dimesions \n",
    "        img = img[:, :, None]\n",
    "    img = tf.Session().run(tf.image.resize_images(img, [200,200])) #Resize the images\n",
    "    return img\n",
    "\n",
    "#We need to create tfrecords. Easier teaching, because its own extension of tensorflow\n",
    "def create_tfrecord_names(tfrecord_path, mode, num_files): #mode (tarin,valid,test), num_files - how much tfrecord should we generate\n",
    "    mode_path = os.path.join(tfrecord_path, mode)\n",
    "    if not os.path.isdir(mode_path): #if there isnt a directory, generate\n",
    "        os.mkdir(mode_path)\n",
    "    return [tf.python_io.TFRecordWriter(os.path.join(mode_path, '{name}.tfrecords'.format(name=index))) for index in range(num_files)] #call the tfrecord writer\n",
    "      \n",
    "   \n",
    "\n",
    "  def create_tfrecords(data_file, tfrecord_path, max_size=5000, train_split=0.8, valid_split=0.1): #max_size: Maximum data in a tfrecord\n",
    "    train_length, valid_length, test_length = divide_data_file(data_file, train_split, valid_split) #call divide_data, we get the shards\n",
    "    data_file_dir = os.path.abspath(os.path.dirname(data_file)) #Source data\n",
    "    \n",
    "    modes = { #3 modes\n",
    "        'train': [  #Create a directory for a train tfrecords\n",
    "            create_tfrecord_names(tfrecord_path, 'train', math.ceil(train_length / max_size)),\n",
    "            os.path.join(data_file_dir, 'train.txt') \n",
    "        ],\n",
    "        'valid': [  #Create a directory for a validation tfrecords\n",
    "            create_tfrecord_names(tfrecord_path, 'valid', math.ceil(valid_length / max_size)),\n",
    "            os.path.join(data_file_dir, 'valid.txt')\n",
    "        ],\n",
    "        'test': [  #Create a directory for a test tfrecords\n",
    "            create_tfrecord_names(tfrecord_path, 'test', math.ceil(test_length / max_size)),\n",
    "            os.path.join(data_file_dir, 'test.txt')\n",
    "        ]\n",
    "        \n",
    "    }\n",
    "             \n",
    "    for mode in modes:\n",
    "        tfrecords_list = modes[mode][0] #There is a list for tfrecords, for every mode\n",
    "        data_file = modes[mode][1] #There is a data_file, for every mode\n",
    "        prev_tfrecords_index = 0\n",
    "        with tqdm.tqdm() as pbar: #It's an indicator, shows us the actual state of the generate\n",
    "            for index, (image_index, text) in enumerate(load_file(data_file)): #Getting the index and the label of the image from load_file\n",
    "                pbar.update() #update the indicator\n",
    "                tfrecords_index = index // max_size #eg: max_size is 5000, we have 14970 data, there will be 3 tf records\n",
    "                example = tf.train.Example( #Need an example and features\n",
    "                    features=tf.train.Features(\n",
    "                        feature={'image': tf.train.Feature(float_list=tf.train.FloatList(value=load_image(image_index).reshape(-1))), #Features are - image and text - so create these pairs \n",
    "                                 'text': tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(text)]))}\n",
    "                ))\n",
    "                tfrecords_list[tfrecords_index].write(example.SerializeToString())\n",
    "                if prev_tfrecords_index != tfrecords_index:\n",
    "                    tfrecords_list[prev_tfrecords_index].close()\n",
    "                    prev_tfrecords_index += 1;\n",
    " \n",
    "        \n",
    "create_tfrecords('D:/Dani/bme/5.félév/dl/hf/output.txt', 'D:/Dani/bme/5.félév/dl/hf/tfrecords')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "data_preprocessing.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
